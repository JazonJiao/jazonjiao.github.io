D5P227-190714
<h2>Technical Summary of my 2019 internship</h2>
This summer, I joined a lab in ISIS (Institute for Software Integrated Systems at Vanderbilt University) lead by Professor Work. Prof. Work’s research focuses on transportation systems and data analytics, as well as autonomous vehicles.
My project team consists of me and two other first-year PhD students. One of them (489) has been working on an algorithm called Robust PCA that can identify the outliers and noise in a dataset, so that we can discard the abnormal outlier data as well as recover the denoised data.
Previously, she only applied the algorithm to a set of traffic data. This summer, we began to test this algorithm on another dataset, from the Array of Things (AoT) project. The AoT project involves installing sensors inside the city to collect data on urban environment.
I am responsible for preprocessing the data. I need to extract data for a certain parameter such as temperature, and organize the data in tensor format, which is basically a multidimensional matrix, with each axis representing, for example, sensor, minute of a day, and day of a year.
In addition, I need to prefilter the data. Since hardware failure is a common issue, there is a lot of “bad” data in the dataset. Sometimes a sensor gives unchanging readings; sometimes a sensor only gives readings for several times a day, instead of every minute, as it should. We found that these kinds of sensors interfere with the algorithm, so it is also my job to filter out these sensors.
After I pre-filtered the data and organized it into the tensor format, 489 tunes the parameters and runs the algorithm. Then we analyze the results: we compare the corrected data with NOAA; when validating our data, I also need to consider the time zone issues.
We also discuss how we could improve the algorithm: Should we use a different cost function? Should we treat missing values as 0 or just missing, or should we interpolate based on other values?
In the first 3 weeks, I took some time to learn a data analysis Python package called pandas, as well as a visualization tool named matplotlib. I only played with a quarter-gigabyte dataset with readings from one day.
Later, as I scaled up my analysis and began analyzing data from across an entire year, I ran into some problems. The entire dataset is more than 204 gigabytes large, and even our lab’s server cannot hold so much data in its RAM. I compressed the dataset to be only 21 GB, so that it can be stored in our server’s RAM and that it takes less time to read.
Sometimes, the server’s RAM is occupied by other teams to run algorithms or data analysis. I then need to communicate with other teams, and when this cannot be reconciled, I need to find ways to process the large data chunk by chunk so that it takes up less memory.
Over time, I gathered a lot of information about the availability, reliability, and trends of data from various parameters. I wrote more than 30 Python programs and plotted more than 200 diagrams to visualize the daily trends of the data, and which nodes are working at what time. I did 4 presentations to present these findings to other professors and lab-mates, and discussed them with the AoT project team in UChicago.
Our lab members sometimes organize meetings for us to study cutting-edge papers in AI fields such as image recognition, or to discuss the progress of each team.
Finally, I wrote a 2700-word report that analyzed the sensor readings and result of algorithm to share with our professor and the AoT team. In all, during the summer, I became proficient at using Python to process and visualize big data, and my research skills improved.
