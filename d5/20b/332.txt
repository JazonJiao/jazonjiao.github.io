D5P332-200724
7.24，我帮xyf解决了一些3250课的问题后，她来我宿舍做了顿韩餐吃，这也是我1个月来首次和别人聚餐。她分享了一些20届同学的去向：zjq去Stanford读生物博士，一位CS女生录了宾大的项目。
直到现在，我对自己入坑NLP的决定还是十分谨慎的。之前探索了半年“AI前景如何”，也没得出什么确定的结论出来…不过哪怕这条路最后没走通，积累人生经验和学习方法也是很好的，参考458。而且我也说过自己不会死磕AI，必要的话还是会学那些Java之类的玩意的。
最近学NLP时总是在想，之前学AI的时候我对知识的理解严重局限于概念，对知识的应用理解极其肤浅、几年都没有太多进步。虽然上次春招时有面试深入考察ML理论，但我还是得多从问题出发才能真正学以致用。
于是，7.24我初步探索了Kaggle。如果能积累一点比赛经验，对我将来申Stanford科研也是极其有用的。为此，有必要分析一下现在我是如何做Leetcode题的。
（1）我对各种数据结构、解题方法都比较熟悉，可以快速排除不合适的思路。
（2）很多时候，我构思的时间远远超过写码的时间，确定解法思路之后再开始写码，一道题3小时构思、20分钟写码是很正常的。
（3）我对自己算法的正确性心中大概有数，明白这个解法为什么可行，即使是有时想出以前从没试过的解法时。
Leetcode里一个算法只可能是正确或者错误，而不像机器学习的任务，准确度是一个连续的值。希望我未来打Kaggle时也能达到刷Leetcode这样的境界，每时每刻大脑都在飞速运转，而不是盲目地调参、试验，或者把大量时间浪费在debug上。
7.26我看了Kaggle上CORD比赛的几个高赞帖子，最高赞是文本聚类，和我去年暑假做的项目还有些相似，感觉整个项目的复杂度和我去年的暑研差不多，可以写一篇小论文了。作者用了很多外部的库，这让我对AI的工程能力有了更深的认识（最近理论学太多了）。
学到兴奋的时候，我又会进入大一时那种自信爆表的状态，但现在的我更了解自己与身边优秀的人的差距，于是这种激动也会更快地平静下来。看着这么多优质的学习材料，真的感觉自己站在巨人的肩膀上。
7.28读了《DL with PyTorch》第9章，感触超级多，知道了ML在应用过程中的问题：数据太大、分工合作难，等等。之前看知乎、公号等关于学界业界脱钩问题时，大都是泛泛而谈，很难让我有直观的理解。而书中则是把来龙去脉都很有逻辑性地讲清楚了。
最近还在想一个问题就是我所学内容的“小众性”。入坑NLP到现在的近4周，我已经搭建起一套多元的学习体系，学习资源包括：（1）S&LP等硬核教材，（2）Stanford的公开课，（3）Kaggle这样的实践训练场，（4）《数学之美》、Computerphile等科普书籍或视频，（5）大佬组织的微信讨论群，以及（6）小夕的公号、知乎等“帖子”类资源。每个的风格、侧重点都有所不同。
正是由于我刚入门时，学的内容都比较“大众化”，才能有这么丰富的学习资源，让我学得这么快。相比而言，我在腾讯实习时做的工作非常“小众”，想想全国能有多少程序员在做路网相关的工作啊。在缺乏指导的情况下，学习资源只有各种难懂的ppt和注释都没有的代码，根本无法系统性地理解。
7.30看224n的作业，每一步都有详细的指示，这让我再次感叹课程和实际的脱钩，业界的工程根本不是这么做的…哎，又要开始批判我直到大二的严重学生思维了，还好现在我已经转变了思维模式！
7.30看224n第8课，想了很多关于机器翻译（MT）可行性的问题。微信上的翻译功能可以说一塌糊涂，但我相信微信团队里都是绝顶聪明的人，如果他们都解决不了，说明有可能是有无法跨越的无解难题的，比如没有训练数据。现在的我对MT兴趣浓厚，但请记住这个任务的需求量（业界岗位数量）并没有我想象的多。
