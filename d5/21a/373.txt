D5P373-210204
<h2>《Recsys Research Project (6)》</h2>
1`2.8，710组织了一次月会，让S2M其他Project的人做分享。有一位教育学PhD在做的项目是：如何评估用户活跃度与阅读能力提升之间的关系（比如引入小测试）。
1`2.8，S2M团队更新了过去3个月的数据（我们的数据量从6个月增加到9个月），此后3天我都在跑已有的清理文件代码。代码里实际上有需要对一个GB级别的dataset进行outer join，结果在AWS上跑，15 GB的内存并不够用；我卡住的时候709就边帮我debug，边去做别的项目了。
1`后来我在Sherlock上跑，但Sherlock不仅一如既往地经常连不上，而且奇怪的是pip install以后Jupyter server里并不会出现对应的软件。2.10问了SRCC解决之后，分配了24 GB的内存，终于成功跑了代码。清理完以后，709在新数据上重跑了一些模型，效果有些提升。
1`显然，以后会有更大量的数据进来，清理数据的代码会更加爆炸，现在的解决方法不是长远之计。当时写这个代码的RA并没有考虑scalability的问题，现在真是要命了。
1`2.4，709说pytorchfm对FM算法的复现有误，我仔细研究了一下发现的确如此，于是709让我试试其他的FM软件。2.11春节放了个假，2.12我尝试了FastFM，然而各种跑不了。之后分别尝试了libFM、xlearn，然而远没有sklearn、pytorch等好用（比如xlearn跑的时候不显示loss、没有built-in的评估metric）。
1`话说2.15，709给了我一个文件夹让我看，结果我总是找不到那个文件夹，浪费了些时间，后来709发现自己写错了，他自己也经常搞不清什么东西放在什么文件夹里…
1`2.16，709说加入用户和文章特征以后，FM的MSE会上升，也就是说“混合过滤”（hybrid filtering，即因子分解机）的表现不如纯协同过滤（PCF，pure collab. filtering，即普通的矩阵分解）。我用不同软件、不同参数在MovieLens上验证了一下，确实如此。这很奇怪，加入更多的信息以后怎么会表现更差呢。
1`709说现在我们有了文章的文本，本来可以计划在推荐模型里加入NLP，但如果看上去引入特征信息以后模型表现更差，还不如只考虑互动信息，那我们得先好好探索一下为啥。
1`另外，“神经协同过滤”（NCF）在S2M上的表现也并不如PCF，于是2.17，709让我研究一下NCF论文，我也写了篇让他挺满意的分析。2.18，我又研究了一下Spotify（KKbox）数据，结果加入特征之后MSE下降了，有意思。
1`2.19，710说我跑出来的csv里面9、10月的数据缺失，让我看看哪里出了问题。我当时很生气，这都啥破事啊。不过后来想想他们也不想这样，归根结底还是回到了整个lab没有设定SWE标准的问题，既没有design doc，也没有code review。709基本负责了主要的代码，如果只是他自己用那没事，但一需要合作就可能出问题。
1`Data cleaning代码是去年秋季lab里另一位RA写的，写得真不怎么样，2千多行里文件名、方法名不知所云；变量名是input但实际代表的是output，output写到和input相同的文件夹；有的文件被基本原封不动地复制了4遍…
1`2.19我搞清了文件处理的流程，知道了这个错误的文件是第3步的产出，上游第1步里有2个相关的文件就已经错了；但并不知道怎么解决。
2`当时我有些担心组员们怀疑我的能力，进而想到我需要更聪明地打造自己的个人名声，证明我的能力，让我在遇到困难时获得更多的话语权和选择权。
1`2.22我终于跑出了正确的数据，次日709验证了加入新数据后模型MSE下降了8%。我说想记录一下清理代码应该怎么跑，709说当时那位RA已经写过文档了，我去，我咋现在才知道？！我看了文档，如果早点看到能省我几小时的功夫啊。不过文档里还是缺失了很多代码运作的重要细节。
1`尤其像这种要跑很多次的、别人需要使用和修改的代码，真的要有很详细的说明，甚至测试文件，不然用起来真是灾难…2.23我写了一篇文档记录预处理代码的行为和使用方法，我个人觉得比原来的文档好很多倍；709看了也觉得很赞，他去修改了一些预处理的代码，我们计划加入更多的sanity checks。
1`我其实想把整个流程重写一遍，当然这样得花很久。产出文件里都有巨量不必要的信息（比如时间精确到0.01毫秒、用户和物品的id用36个字符表示、每个用户每天都记录一些信息…），但如果我尝试去掉它们，估计会break很多其他的代码，因为似乎有很多代码都依赖于现在的格式。哎，这些在搭建软件初期的设计决定（design choices），之后再要推翻得花多大的代价啊。
（上篇：D5P372，下篇：D5P379）
