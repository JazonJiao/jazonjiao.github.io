D5P380-210307
2`3.4晚上第1次参加981组织的TeckTok（300+人参加），她的团队请到了TuSimple的CTO侯晓迪，讲得太棒了，瞬间圈粉！他提到“投入产出比”的问题，不同的路段的成本不同；卡车要应对的任务比轿车更专一（770提过）；如何在没有信号的路段保证安全；今年计划去掉安全员，问题不在算法，而在硬件（硬件主要来自供应商合作）。
2`他提到“和技术栈作斗争”：如何组织公司的架构、模块化软件，以最低的成本应对新的小需求。还谈到公司基因：有的公司“以人为本”，看重公司结构、员工升职。TuSimple是以技术为本，重工程。总的来说，他的观点实在太深了，我很多都还不能理解…
2`3.5和上届MSCS学姐I初次见面打了网球。她在Stanford做过CV、Bitcoin方面的科研，这学期在MSE做ethics方向的RA，4月开始在Google上班。我说我本科3年，她评价说Stanford喜欢这种聪明人hhh，还说她一届的一位学长是1998.5生的。
1`3.3晚上我和966去Andrew Wang（分给我们的TA）OH求教，不过他也没给啥实质性的建议。这当然不怪他，毕竟一位TA要mentor十几个team…3.5凌晨我向479吐槽模型一开始train，F1就断崖下跌，问她会不会是因为overfit。她看了觉得不太可能，应该是bug。之后又讨论了一些可以尝试的模型：Longformer、Graph。
1`一开始我不认为哪里出了bug；3.6凌晨我尝试只在Molweni上fine-tune，效果很好，才让我感觉是有bug。3:30，我终于意识到了问题所在，原来是我答案的开始index算错了！我以为是按word位置算，原来是按char位置算。这个bug并不好找，因为evaluate时计算准确率并没有问题。
1`这时我已经跑了十几次实验，这些实验只要涉及FriendsQA都废掉了…之后一开始还是不对，原来是我没有重新产出训练集的pt文件。注：479说BERT不太擅长长文本，所以我重新产出train set时，去掉了词数大于896的context，dev和test不动。
1`之后我和966分别研究了怎么在BERT里加入Utterance-level Embedding（ULE），但最后得出的结论是，我们不太可能发明什么其他模型结构或者训练方法，比那篇paper做得更好了。966可以跑跑他们的代码，但我们肯定不能单纯只跑别人代码。当然，那篇paper因为篇幅限制，省略了超多细节，也许我们可以在Project里补全这些细节。
1`3.7晚上，我们还是决定放弃ULE这条路。我们讨论了RobustQA里的思路，感觉都不太可行，其他做RobustQA的同学试了以后效果都反而变差了。最后决定接下来一周搞数据增强（对应handout里5.3 Robustness via Data Augmentation），搞好以后最后一周可以冲刺刷分。所以离due还有不到12天，我们又换了目标…
1`966好奇为啥没人做莎士比亚话剧的QA，不愧是英语双专业。她和我吐槽231a课程设置混乱，比如做附加分的人太多，导致有人抗议这是变相强制做附加分；给的Azure credits太少等。
1`话说我看了RobustQA里“5.5 Few-sample Fine-tuning”的对应paper，得分最高的方法是先在更大的数据上训练，再在小数据上训练。当然做RobustQA的人是绝不能使用另外的数据集的，就很蛋疼。
1`话说之前做实验，我发现把5个训练集合成起来训练，效果好于只在FriendsQA上训练。于是我想能不能先训练5个dataset，再在此基础上训练FriendsQA。我觉得这从梯度下降的角度是合理的，但实验出来是失败了。
1`3.10凌晨我不困，就一夜没睡，狂肝back-translation。这期间我一直在想答案匹配的问题，比较word embedding的平均值似乎不太行；突然有一刻我突发奇想，为什么我不把embedding加起来比较呢？这样考虑进了匹配句子的长度。3.10傍晚狂肝了一阵子写出来了这个方法（解决了不少nasty bugs），效果还不错。而且我找了一圈没发现有人做过，激动！这样的小创新让我很有成就感。
1`CS 142第6、7个Project我各写了约20小时。3.8我注册了参加Vandy的20届毕业典礼。3.7选课，770、776、774没抢到高尔夫（PE 33），966说golf和“Social Dance”是Stanford最难抢的课。3.9发现我迷之选不上CS 110。
