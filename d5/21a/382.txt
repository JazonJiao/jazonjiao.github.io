D5P382-210311
1`3.11拿到150刀Azure credits后，我搞了个4 GPU（45 GB内存）的VM，想在GPU上跑BERT-large，结果还是内存不够。于是我狂肝了6小时，先是写代码把训练集拆开，然后train.py里加torch.nn.DataParallel，完了又得把save_pretrained() 改成torch.save()，还要保存model.state_dict()（感谢作业5让我有了个参照）。令人崩溃的是reload以后还是会内存不够。
1`论坛上不少其他同学也遇到了相同的问题。3.14我发现reload后崩溃的原因是GPU不会自动清内存，哪怕用del删除变量，哪怕gc.collect() 然后torch.cuda.empty_cache() 也不够。
1`于是我想直接写个shell script不停地从cmd来reload，结果又折腾了好久（中间遇到各种问题：str没转化为int、loss.sum() 应该是loss.mean() 否则学习率相当于 × 4），才以非常蛋疼的方式跑起来。（最后还是没成功）
1`话说在不会搞多GPU训练的时候，我就用CPU跑BERT-large。一开始跑的是在5个数据集上train，贼慢，2小时才eval一次，然后我就像炒股一样，天天盯着loss和F1是涨了还是跌了…dev F1在训练开始2天后达到顶峰（68.01），此后一直飘忽不定。3.12突然loss爆炸、F1暴跌，于是我停掉了训练，跑了5天的模型就这样废掉了。
1`Project做到这里我已经快做吐了，不过看着离SoTA（F1 = 69.6，EM = 53.5）越来越近，还是很激动。需要动力的时候就看看carykh的CS 221、230 Project视频。之后我跑回翻增强的数据，即使还是在CPU上，但很快表现就超过了5个数据集上train的模型。
1`至3.12晚上，数据增强的工作和distilbert实验已经基本完成，剩下的就是跑bert-large和ensemble了。Ensemble最简单的是每个test output比较，不过我想把top K的score和pred都加起来再比较，结果又花了我好久，无论从头开始写，还是改它的代码，都很复杂…
1`结果试了几次，ensemble的结果都是平均了几个model的表现，而不是增强。后来发现我之前test F1记录得有问题，评估的是train完以后的model，而ensemble用的保存的model，test F1更低。奇怪的是，保存的model的dev F1是更高的。
1`重新计算之后，ensemble确实带来了分数提升。3.14晚上21:46，我结合了两个BERT-large，F1突破了70.3，这是历史性的一刻！虽然我相当于用了比原SoTA模型多5倍的参数，但作为NLP新手，中间有太多步骤可以出错（比如搞不出分布式训练），能做到这样已经不容易啦。
2`966下学期变成硕士生status了，她吐槽这学期忙爆炸（主要是又做TA又做RA），根本来不及思考人生。看她这么忙我好心疼，感叹我比她多好多倍的时间可以花在project上…
3`3.12过22岁生日，下午生日party请了564、770、729、774、776，中间出了一些小插曲，又是外卖送错又是把自己锁门外，不过都很欢乐。感谢774的点蜡烛神器、美图。
2`今年的生日pyq和2019年是一样的格式，概述了这一年的心路历程，我花了很久构思，今年年初就开始攒背景图片；我1/3的好友都点了赞！大学期间由于493，我经常把年龄多记1岁，现在的我则不会记错，因为我总提醒自己比别人小1岁XD。
