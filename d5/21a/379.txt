D5P379-210227
<h2>《Recsys Research Project (7)》</h2>
1`2月末，我又在KKBox上做了很多实验，比较了HF、ClF、CtF的表现，然后下采样（downsample）到S2M的数据量重复实验（中间还搞错了如何做downsample）。虽然我知道了HF > ClF > CtF，但我并没有深入理解为啥会这样，这是“战略上的懒惰”。
1`3.3，我们几周来第一次和Susan开组会。一开始大家讲一些统计和经济的东西我没太听懂；之后710问Susan对下一步的方向有何建议。让我意外的是，Susan说她主要指望学生们自己头脑风暴，给作为导师的她提供灵感；我们学生应该比她更清楚怎样的project更publishable、能推进科学发展、有益于合作伙伴。我们应该主动提出project，然后从这些维度评估可行性。
1`3.3晚上，709也说我应该对project建立完全的自主权（他常不经意地说“我想让你做xxx”，他说这不对，应该是“你可以做xxx”）。确实，入lab三个月，我已经发展了很多自己的想法。但与此相矛盾的是，我提出的很多想法都会被709或Susan否决：要么是别人已经做过了，要么没必要，要么不现实。
1`比如我说用Neural CF，“我们连基本的CF都没搞好，Neural CF怎么会更好呢”；比如我说加入多样性，“现在的问题不是多样性，不是说推荐物品卡在某种类别上”；比如我说我可以用分布式数据处理工具，重写预处理流程，“这不是我们的任务，公司工程团队做不出来的才应该由我们做”。当然，709说他的idea被Susan否决也是家常便饭。
2`709开诚布公地对我说，“如果你觉得我哪里不好，你可以单独向710吐槽”。我的队员有如此胸怀，和他们共事好棒！还问我喜欢和不喜欢project的什么方面、我希望这个RAship如何fit进我的职业规划，让我放开说。
1`我说喜欢探索给用户、物品建模的过程；不喜欢的地方是，老让我算RMSE就很鸡肋，不能真正反映推荐系统的好坏。709说确实如此，他举了一个例子，说明一个模型做出了正确的推荐但RMSE还是会很高：准确预测了某个用户评分的相对名次，但绝对数值错误，普遍上移或者下移。
1`现在我对下面几个月要做啥还是挺迷茫的。我说强烈希望能自己搭建一套完整的推荐系统，从csv数据开始，产出两个模块（TFY、RS）的推荐结果，而不是天天算RMSE。虽然709已经做了，但说不定我自己能提供新思路呢。而且（1）这个更能帮助我的职业发展，（2）我也不知道应该做什么其他事情了…
1`最近我们探讨的最大问题是如何应对用户和物品的冷启动（cold start）问题。我们可以按popularity给新用户推荐，但问题是从什么时候开始个性化呢。709让我读一读Hierarchical Modeling，这种方法使用某种“先验分布”（prior distr.）来解决新用户信息不足的问题，不过我们还是放弃了这种方法。
1`我还研究了一下离线评估指标的问题，和709提出使用MAE而不是RMSE、把Freadom推荐模型的utility换成二元值，不过都没有被采纳。还有怎么创建测试集的问题，还有如何理解latents的意义，还有算法的时间复杂度。
1`3.10，我们又提出了一个基于排名准确率的指标，precision@k。计算的是给预测的utility排序之后，前k个实际utility是“正样本”的比例。
1`显然，如果数据里有很多用户只有不到k个interactions，那这个metric就没有意义了，所以需要过滤掉互动数量小于k的用户，好在709已经做好了这一步。还有的问题是需不需要单独split出测试集来计算；现在我们并没有区分train/test。
1`3.15，我实现了这个的代码，之后做实验，看各种模型和baseline在precision@k上的表现。709期望我能很快完成，然而最后直到3.24才做出来（中间放了一周假），实际花了我3天时间。
1`原因在于他的ItemMeans模型本来不是用PyTorch写的，我得把它变成PyTorch的class；那以后出了各种难解的bug，又是train命名冲突，又是forward的output不对，又是0D tensor作为getitem的参数会有奇怪的事情发生。搞好以后，发现我们的个性化推荐模型，确实表现更佳。
（上篇：D5P373，下篇：D5P381）
